
[![PythonVersion](https://img.shields.io/pypi/pyversions/gino_admin)](https://img.shields.io/pypi/pyversions/gino_admin)
[![tests](https://github.com/datarootsio/ml-skeleton-py/workflows/tests/badge.svg?branch=master)](https://github.com/datarootsio/ml-skeleton-py/actions)
[![Code style: black](https://img.shields.io/badge/code%20style-black-000000.svg)](https://github.com/psf/black)
[![PRs Welcome](https://img.shields.io/badge/PRs-welcome-brightgreen.svg?style=flat-square)](http://makeapullrequest.com)
![](https://scontent.fbru1-1.fna.fbcdn.net/v/t1.0-9/94305647_112517570431823_3318660558911176704_o.png?_nc_cat=111&_nc_sid=e3f864&_nc_ohc=-spbrtnzSpQAX_qi7iI&_nc_ht=scontent.fbru1-1.fna&oh=483d147a29972c72dfb588b91d57ac3c&oe=5F99368A "Logo")


# `Caption the internet`


## Objective

Generate synthetic captions for images, audio and videos to create better datasets.

## Usage

### Benchmark different captioner model to select best ones: [benchmark.sh](scripts/benchmark.sh)

You can examine how many resources (gpu memory, number of gpu hrs) you would need to caption images using set of models and how good those captioners are (e.g. using ClipScore). See [benchmark_config.json](scripts/benchmark_config.json) for details.

![image](https://github.com/marianna13/caption-the-internet/assets/43296932/6e26bf9a-f270-4122-be74-90acb7b005ec)



